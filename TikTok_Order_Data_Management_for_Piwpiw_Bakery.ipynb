{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TikTok Order Data Management for Piwpiw Bakery\n",
    "\n",
    "## 1. Project Overview\n",
    "\n",
    "**Objective:**  \n",
    "Develop a comprehensive system for managing TikTok orders at Piwpiw Bakery, addressing challenges related to data storage, order tracking, and handling returns.\n",
    "\n",
    "## 2. Requirements Analysis\n",
    "\n",
    "**Business Requirements:**  \n",
    "- Store TikTok order data and labels.\n",
    "- Track users with a history of parcel returns.\n",
    "- Automate data entry, updating, and reporting.\n",
    "\n",
    "**Technical Requirements:**  \n",
    "- Google Sheets and Google Drive APIs for data storage.\n",
    "- Python for scripting and automation.\n",
    "- ETL process for data transformation.\n",
    "\n",
    "## 3. Design Specifications\n",
    "\n",
    "**a. Data Collection and Storage**\n",
    "\n",
    "- **Data Sources:**  \n",
    "  TikTok orders, PDF labels.\n",
    "\n",
    "- **Data Extraction:**  \n",
    "  Use PyMuPDF for extracting labels from PDFs.\n",
    "  \n",
    "- **Data Storage:**  \n",
    "  Store extracted data in Google Sheets.\n",
    "\n",
    "- **APIs:**  \n",
    "  - Google Sheets API for data entry and retrieval.\n",
    "  - Google Drive API for file management.\n",
    "\n",
    "**b. ETL Process**\n",
    "\n",
    "- **Extract:**  \n",
    "  Extract data from PDF labels and existing Google Sheets.\n",
    "\n",
    "- **Transform:**  \n",
    "  Process and clean data:\n",
    "  - Check for and handle duplicate order IDs.\n",
    "  - Merge new data with existing records.\n",
    "\n",
    "- **Load:**  \n",
    "  Update Google Sheets with processed data.\n",
    "\n",
    "**c. Workflow Automation**\n",
    "\n",
    "- **Data Entry and Updating:**  \n",
    "  Develop Python scripts to automate:\n",
    "  - Data extraction from PDFs.\n",
    "  - Data entry and updates in Google Sheets.\n",
    "  - Consistency checks across datasets.\n",
    "\n",
    "**d. Order Monitoring and Reporting**\n",
    "\n",
    "- **Order Tracking:**  \n",
    "  Script to:\n",
    "  - Monitor new orders.\n",
    "  - Identify users with a history of parcel returns.\n",
    "\n",
    "- **Automated Reporting:**  \n",
    "  Script to:\n",
    "  - Generate reports on problematic orders.\n",
    "  - Send automated notifications to Piwpiw Bakery for follow-up.\n",
    "\n",
    "## 4. Implementation Plan\n",
    "\n",
    "**a. Development:**\n",
    "\n",
    "- **Phase 1: Setup**\n",
    "  - Configure APIs (Google Sheets, Google Drive).\n",
    "  - Install necessary Python libraries (PyMuPDF, gspread).\n",
    "\n",
    "- **Phase 2: Data Extraction and Storage**\n",
    "  - Develop and test scripts for extracting data from PDFs.\n",
    "  - Implement Google Sheets integration.\n",
    "\n",
    "- **Phase 3: ETL Process**\n",
    "  - Create and test ETL scripts for data transformation.\n",
    "\n",
    "- **Phase 4: Automation**\n",
    "  - Develop scripts for data entry, updating, and consistency maintenance.\n",
    "\n",
    "- **Phase 5: Order Monitoring and Reporting**\n",
    "  - Develop and test scripts for order tracking and reporting.\n",
    "\n",
    "**b. Testing:**\n",
    "\n",
    "- **Unit Testing:**  \n",
    "  Test individual scripts and functions.\n",
    "\n",
    "- **Integration Testing:**  \n",
    "  Ensure all components work together seamlessly.\n",
    "\n",
    "- **User Acceptance Testing:**  \n",
    "  Validate with Piwpiw Bakery to ensure requirements are met.\n",
    "\n",
    "**c. Deployment:**\n",
    "\n",
    "- **Deploy Scripts:**  \n",
    "  Deploy and schedule Python scripts on a server or cloud platform.\n",
    "\n",
    "- **Training:**  \n",
    "  Train bakery staff on using the new system.\n",
    "\n",
    "## 5. Maintenance and Support\n",
    "\n",
    "- **Monitoring:**  \n",
    "  Regularly monitor system performance and data accuracy.\n",
    "\n",
    "- **Updates:**  \n",
    "  Implement updates and improvements based on feedback.\n",
    "\n",
    "- **Support:**  \n",
    "  Provide ongoing support and troubleshooting as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gspread\n",
      "  Using cached gspread-6.1.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauth2client\n",
      "  Using cached oauth2client-4.1.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\banas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\banas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.24.1)\n",
      "Requirement already satisfied: google-auth>=1.12.0 in c:\\users\\banas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gspread) (2.23.0)\n",
      "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in c:\\users\\banas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gspread) (1.0.0)\n",
      "Collecting httplib2>=0.9.1 (from oauth2client)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in c:\\users\\banas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from oauth2client) (0.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in c:\\users\\banas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from oauth2client) (0.3.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in c:\\users\\banas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from oauth2client) (4.9)\n",
      "Requirement already satisfied: six>=1.6.1 in c:\\users\\banas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from oauth2client) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\banas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\banas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\banas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth>=1.12.0->gspread) (5.3.1)\n",
      "Collecting urllib3<2.0 (from google-auth>=1.12.0->gspread)\n",
      "  Downloading urllib3-1.26.19-py2.py3-none-any.whl.metadata (49 kB)\n",
      "     ---------------------------------------- 0.0/49.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 49.3/49.3 kB 2.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\banas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth-oauthlib>=0.4.1->gspread) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\banas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httplib2>=0.9.1->oauth2client) (3.0.9)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\banas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.2)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\banas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\banas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\banas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\banas\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2024.2.2)\n",
      "Using cached gspread-6.1.2-py3-none-any.whl (57 kB)\n",
      "Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading urllib3-1.26.19-py2.py3-none-any.whl (143 kB)\n",
      "   ---------------------------------------- 0.0/143.9 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 61.4/143.9 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 143.9/143.9 kB 1.7 MB/s eta 0:00:00\n",
      "Installing collected packages: urllib3, httplib2, oauth2client, gspread\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.1\n",
      "    Uninstalling urllib3-2.2.1:\n",
      "      Successfully uninstalled urllib3-2.2.1\n",
      "Successfully installed gspread-6.1.2 httplib2-0.22.0 oauth2client-4.1.3 urllib3-1.26.19\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyppeteer 2.0.0 requires websockets<11.0,>=10.0, but you have websockets 12.0 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.9.0 which is incompatible.\n",
      "torchvision 0.15.2 requires torch==2.0.1, but you have torch 2.1.2 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install gspread oauth2client pandas numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication and Connection\n",
    "\n",
    "Authenticates using OAuth2 credentials and connects to Google Sheets using the gspread library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to your service account key file\n",
    "CLIENT_SECRET_FILE = os.getenv('CLIENT_SECRET_FILE')\n",
    "\n",
    "API_NAME = 'drive'\n",
    "API_VERSION = 'v3'\n",
    "# Scopes for the APIs\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for Duplicate Order IDs and Updating Orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETL Process Overview: \n",
    "\n",
    "Extract: Data is pulled from various sources, including multiple Google Sheets and Google Drive.\n",
    "\n",
    "Transform: Data is cleaned and filtered to identify non-duplicate records, ensuring data quality and consistency.\n",
    "\n",
    "Load: The processed data is stored in target Google Sheets, and the process is logged for auditing and tracking purposes.\n",
    "By integrating these phases, the script helps maintain a clean, up-to-date dataset, essential for further analysis and decision-making processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "import pandas as pd\n",
    "from google.oauth2.service_account import Credentials\n",
    "from datetime import datetime\n",
    "from googleapiclient.discovery import build\n",
    "import os\n",
    "\n",
    "\n",
    "# Create a credentials object\n",
    "creds = Credentials.from_service_account_file(CLIENT_SECRET_FILE, scopes=SCOPES)\n",
    "\n",
    "# Initialize gspread client\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Initialize Google Drive client\n",
    "drive_service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "def get_sheet_data(sheet_url, sheet_name, file_name):\n",
    "    \"\"\"Fetch data from a Google Sheet and add file name to the DataFrame.\"\"\"\n",
    "    try:\n",
    "        sheet = client.open_by_url(sheet_url).worksheet(sheet_name)\n",
    "        print(f\"Sheet name: {sheet.title}\")\n",
    "        records = sheet.get_all_records()\n",
    "        if not records:\n",
    "            print(f\"Sheet {sheet_name} is empty.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        df = pd.DataFrame(records)\n",
    "        df['File Name'] = file_name\n",
    "        return df\n",
    "    except gspread.exceptions.APIError as e:\n",
    "        print(f\"APIError fetching data from {sheet_name}: {e}\")\n",
    "    except gspread.exceptions.WorksheetNotFound as e:\n",
    "        print(f\"WorksheetNotFound error fetching data from {sheet_name}: {e}\")\n",
    "    except gspread.exceptions.SpreadsheetNotFound as e:\n",
    "        print(f\"SpreadsheetNotFound error fetching data from {sheet_name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"General error fetching data from {sheet_name}: {e}\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def write_to_sheet(sheet_url, sheet_name, data):\n",
    "    \"\"\"Write data to a Google Sheet.\"\"\"\n",
    "    try:\n",
    "        sheet = client.open_by_url(sheet_url).worksheet(sheet_name)\n",
    "        sheet.clear()\n",
    "        if not data.empty:\n",
    "            # Convert data to a list of lists and ensure numbers are formatted as strings\n",
    "            formatted_data = data.applymap(lambda x: f\"{x:.0f}\" if isinstance(x, (int, float)) else x)\n",
    "            # Update the sheet with column headers and data\n",
    "            sheet.update([formatted_data.columns.values.tolist()] + formatted_data.values.tolist())\n",
    "        else:\n",
    "            print(f\"No data to write to {sheet_name}.\")\n",
    "    except gspread.exceptions.APIError as e:\n",
    "        print(f\"APIError writing data to {sheet_name}: {e}\")\n",
    "    except gspread.exceptions.WorksheetNotFound as e:\n",
    "        print(f\"WorksheetNotFound error writing data to {sheet_name}: {e}\")\n",
    "    except gspread.exceptions.SpreadsheetNotFound as e:\n",
    "        print(f\"SpreadsheetNotFound error writing data to {sheet_name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"General error writing data to {sheet_name}: {e}\")\n",
    "\n",
    "def compare_order_ids_and_filter(master_df, update_df):\n",
    "    \"\"\"Compare Order IDs and return non-duplicate rows with Date and Timestamp, and count of orders checked.\"\"\"\n",
    "    # Clean 'Order ID' columns in both DataFrames\n",
    "    master_df['Order ID'] = master_df['Order ID'].astype(str).str.strip()\n",
    "    update_df['Order ID'] = update_df['Order ID'].astype(str).str.strip()\n",
    "    \n",
    "    # Remove header rows if present in the 'Order ID' column\n",
    "    master_df = master_df[master_df['Order ID'] != 'Platform unique order ID.']\n",
    "    update_df = update_df[update_df['Order ID'] != 'Platform unique order ID.']\n",
    "    \n",
    "    # Print unique Order IDs for debugging\n",
    "    print(f\"Unique Order IDs in Master Sheet: {master_df['Order ID'].unique()}\")\n",
    "    print(f\"Unique Order IDs in Update Sheet: {update_df['Order ID'].unique()}\")\n",
    "    \n",
    "    # Create a set of 'Order ID' from master_df for faster lookup\n",
    "    master_ids = set(master_df['Order ID'])\n",
    "    \n",
    "    # Filter out rows in update_df where 'Order ID' is not in master_ids\n",
    "    non_duplicates = update_df[~update_df['Order ID'].isin(master_ids)]\n",
    "    \n",
    "    # Print the filtered non-duplicate Order IDs for debugging\n",
    "    print(f\"Non-Duplicate Order IDs: {non_duplicates['Order ID'].unique()}\")\n",
    "    \n",
    "    # Add 'Date' and 'Timestamp' columns\n",
    "    non_duplicates['Date Check Update'] = datetime.now().strftime('%Y-%m-%d')\n",
    "    non_duplicates['Timestamp Check Update'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Ensure unique 'Order ID' values in non_duplicates\n",
    "    non_duplicates = non_duplicates.drop_duplicates(subset='Order ID')\n",
    "    \n",
    "    return non_duplicates, len(update_df)\n",
    "\n",
    "def log_update(sheet_url, sheet_name, file_name, order_count, non_duplicate_count):\n",
    "    \"\"\"Log the file name, date, timestamp, and count of orders checked and non-duplicate orders to the update log sheet.\"\"\"\n",
    "    try:\n",
    "        # Open the spreadsheet by URL\n",
    "        spreadsheet = client.open_by_url(sheet_url)\n",
    "        print(f\"Opened spreadsheet: {spreadsheet}\")\n",
    "        \n",
    "        # Try to open the worksheet by name\n",
    "        try:\n",
    "            sheet = spreadsheet.worksheet(sheet_name)\n",
    "            print(f\"Accessed worksheet: {sheet.title}\")\n",
    "        except gspread.exceptions.WorksheetNotFound:\n",
    "            print(f\"Worksheet '{sheet_name}' not found in the spreadsheet.\")\n",
    "            # Optionally, list all sheet names to verify\n",
    "            sheet_names = [ws.title for ws in spreadsheet.worksheets()]\n",
    "            print(f\"Available sheets: {sheet_names}\")\n",
    "            return\n",
    "        \n",
    "        # Fetch existing data\n",
    "        existing_data = sheet.get_all_records()\n",
    "        log_df = pd.DataFrame(existing_data)\n",
    "        \n",
    "        # Create a new log entry\n",
    "        new_entry = {\n",
    "            'File Name': file_name,\n",
    "            'Date': datetime.now().strftime('%Y-%m-%d'),\n",
    "            'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'Order Count': order_count,\n",
    "            'Non-Duplicate Order Count': non_duplicate_count\n",
    "        }\n",
    "        \n",
    "        # Append new entry to DataFrame using pd.concat\n",
    "        new_entry_df = pd.DataFrame([new_entry])\n",
    "        log_df = pd.concat([log_df, new_entry_df], ignore_index=True)\n",
    "        \n",
    "        # Fill NaN values with an empty string to avoid JSON issues\n",
    "        log_df = log_df.fillna('')\n",
    "        \n",
    "        # Update the worksheet with new data\n",
    "        print(f\"Log DataFrame to be written:\\n{log_df}\")  # Debug statement\n",
    "        sheet.update([log_df.columns.values.tolist()] + log_df.values.tolist())\n",
    "        print(f\"Logged update for {file_name} with {order_count} orders checked and {non_duplicate_count} non-duplicate orders.\")\n",
    "    \n",
    "    except gspread.exceptions.APIError as e:\n",
    "        print(f\"APIError logging update for {file_name}: {e}\")\n",
    "    except gspread.exceptions.SpreadsheetNotFound as e:\n",
    "        print(f\"SpreadsheetNotFound error logging update for {file_name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"General error logging update for {file_name}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def get_files_in_folder(folder_id):\n",
    "    \"\"\"Get list of files in a Google Drive folder.\"\"\"\n",
    "    try:\n",
    "        results = drive_service.files().list(\n",
    "            q=f\"'{folder_id}' in parents and mimeType='application/vnd.google-apps.spreadsheet'\",\n",
    "            fields=\"files(id, name)\"\n",
    "        ).execute()\n",
    "        files = results.get('files', [])\n",
    "        print(f\"Files found: {files}\")  # Print the files retrieved\n",
    "        return files\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting files from folder: {e}\")\n",
    "        return []\n",
    "\n",
    "def process_google_sheet(file_id, file_name):\n",
    "    \"\"\"Process a Google Sheet directly from Drive.\"\"\"\n",
    "    try:\n",
    "        # Construct the URL for the sheet\n",
    "        sheet_url = f\"https://docs.google.com/spreadsheets/d/{file_id}/edit\"\n",
    "        # Fetch the sheet data (assuming the first sheet/tab)\n",
    "        sheet = client.open_by_url(sheet_url).sheet1\n",
    "        records = sheet.get_all_records()\n",
    "        if not records:\n",
    "            print(f\"Sheet with ID {file_id} is empty.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        df = pd.DataFrame(records)\n",
    "        df['File Name'] = file_name  # Add the file name to the DataFrame\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Google Sheet with ID {file_id}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# URLs of the Google Sheets (Replace with actual URLs)\n",
    "MASTER_SHEET_URL = os.getenv('MASTER_SHEET_URL')\n",
    "OUTPUT_SHEET_URL = os.getenv('OUTPUT_SHEET_URL')\n",
    "LOG_SHEET_URL = os.getenv('LOG_SHEET_URL')\n",
    "\n",
    "# Sheet names (tabs) in the Google Sheets\n",
    "MASTER_SHEET_NAME = 'Master Sheet'\n",
    "OUTPUT_SHEET_NAME = 'Update Order For Check Blacklist'\n",
    "LOG_SHEET_NAME = 'Update Log'\n",
    "\n",
    "# Google Drive folder ID containing the update files (replace with your folder ID)\n",
    "UPDATE_FOLDER_ID = os.getenv('UPDATE_FOLDER_ID')\n",
    "# Fetch data from master sheet\n",
    "master_df = get_sheet_data(MASTER_SHEET_URL, MASTER_SHEET_NAME, 'Master Sheet')\n",
    "\n",
    "if master_df.empty:\n",
    "    print(f\"Failed to fetch data from {MASTER_SHEET_NAME}.\")\n",
    "else:\n",
    "    # Get list of files in the update folder\n",
    "    files = get_files_in_folder(UPDATE_FOLDER_ID)\n",
    "    if not files:\n",
    "        print(f\"No files found in folder with ID {UPDATE_FOLDER_ID}.\")\n",
    "    else:\n",
    "        for file in files:\n",
    "            file_id = file['id']\n",
    "            file_name = file['name']\n",
    "            \n",
    "            print(f\"Processing file: {file_name} (ID: {file_id})\")\n",
    "            \n",
    "            # Process the Google Sheet directly\n",
    "            update_df = process_google_sheet(file_id, file_name)\n",
    "            \n",
    "            if update_df is not None and not update_df.empty:\n",
    "                # Compare Order IDs and get non-duplicate rows\n",
    "                non_duplicates_df, order_count = compare_order_ids_and_filter(master_df, update_df)\n",
    "                \n",
    "                non_duplicate_count = len(non_duplicates_df)\n",
    "                \n",
    "                if not non_duplicates_df.empty:\n",
    "                    # Write non-duplicate rows to output sheet\n",
    "                    write_to_sheet(OUTPUT_SHEET_URL, OUTPUT_SHEET_NAME, non_duplicates_df)\n",
    "                    print(f\"Non-duplicate rows from {file_name} have been written to {OUTPUT_SHEET_NAME}.\")\n",
    "                \n",
    "                # Log the update\n",
    "                log_update(LOG_SHEET_URL, LOG_SHEET_NAME, file_name, order_count, non_duplicate_count)\n",
    "                print(f\"Logged update for {file_name} with {order_count} orders checked and {non_duplicate_count} non-duplicate orders.\")\n",
    "            else:\n",
    "                print(f\"Failed to process file {file_name}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and Processing Data from PDF Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script implements an ETL (Extract, Transform, Load) process to extract data from PDF labels stored in Google Drive, transform the data to extract relevant fields, and load the results into a Google Sheets document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Phase:\n",
    "\n",
    "Google Drive File Retrieval: The script connects to Google Drive using the API, lists all files in a specified folder (folder_id), and retrieves them, handling pagination as needed.\n",
    "PDF Text Extraction: For each PDF, it downloads the content, extracts the full text using PyMuPDF (fitz), and stores the text in a single string for further processing.\n",
    "\n",
    "Transform Phase:\n",
    "\n",
    "Data Structuring: The extracted information is organized into a DataFrame with columns for Name, Address, Order ID, Phone, Product, Date, and Timestamp.\n",
    "Filtering Duplicate Entries: The script removes duplicate Order IDs by comparing them with existing IDs in the \"Label Extract MasterSheet\" on Google Sheets.\n",
    "\n",
    "Load Phase:\n",
    "\n",
    "Data Appending: Non-duplicate data is appended to the \"Label Extract MasterSheet\" in Google Sheets using the gspread library.\n",
    "Logging and Monitoring: Status messages are printed throughout the process, tracking progress and facilitating debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "import io\n",
    "import gspread\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to extract text from all pages of a PDF from a stream\n",
    "def extract_text_from_pdf_stream(pdf_stream):\n",
    "    document = fitz.open(\"pdf\", pdf_stream)\n",
    "    full_text = \"\"\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        page_text = page.get_text()\n",
    "        full_text += page_text\n",
    "    return full_text\n",
    "\n",
    "# Function to split address into components\n",
    "def split_address(address):\n",
    "    parts = address.split(',')\n",
    "    address_parts = [part.strip() for part in parts]\n",
    "    # Ensure we have at least 7 components, fill with empty strings if less\n",
    "    address_parts += [''] * (7 - len(address_parts))\n",
    "    return address_parts[:7]\n",
    "\n",
    "# Function to remove spaces from address components\n",
    "def remove_spaces(address_part):\n",
    "    return address_part.replace(\" \", \"\")\n",
    "\n",
    "# Function to extract name, address, order ID, phone, and products from DataFrame\n",
    "def extract_name_address_order_phone_and_products(df):\n",
    "    name = None\n",
    "    address_lines = []\n",
    "    order_id = None\n",
    "    phone = None\n",
    "    products = []\n",
    "    extracting_products = False\n",
    "    extracted_data = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        lines = row['pdf_extract'].split('\\n')\n",
    "        for line in lines:\n",
    "            if \"ถึง\" in line:\n",
    "                try:\n",
    "                    name = lines[lines.index(line) + 1].strip()\n",
    "                    for address_line in lines[lines.index(line) + 2:]:\n",
    "                        if \"EZ\" in address_line:\n",
    "                            break\n",
    "                        address_lines.append(address_line.strip())\n",
    "                except IndexError:\n",
    "                    pass\n",
    "            elif \"Order ID:\" in line:\n",
    "                try:\n",
    "                    order_id = line.split(\"Order ID:\")[1].strip()\n",
    "                except IndexError:\n",
    "                    pass\n",
    "            elif \"(+66)\" in line:\n",
    "                try:\n",
    "                    phone = line.strip()\n",
    "                except IndexError:\n",
    "                    pass\n",
    "            elif \"Product Name\" in line:\n",
    "                extracting_products = True\n",
    "                continue\n",
    "            elif \"Qty Total\" in line:\n",
    "                extracting_products = False\n",
    "                continue\n",
    "            elif extracting_products:\n",
    "                products.append(line.strip())\n",
    "        if name and products:\n",
    "            full_address = ' '.join(address_lines).strip()\n",
    "            address_parts = split_address(full_address)\n",
    "            address_no_space_parts = [remove_spaces(part) for part in address_parts]\n",
    "            extracted_data.append((\n",
    "                name,\n",
    "                full_address,\n",
    "                address_parts[0],  # Address1\n",
    "                address_parts[1],  # Address2\n",
    "                address_no_space_parts[1],  # Address2 No space\n",
    "                address_parts[2],  # Address3\n",
    "                address_no_space_parts[2],  # Address3 No space\n",
    "                address_parts[3],  # Address4\n",
    "                address_no_space_parts[3],  # Address4 No space\n",
    "                address_parts[4],  # Address5\n",
    "                address_parts[5],  # Address6\n",
    "                address_parts[6],  # Address7\n",
    "                order_id,\n",
    "                phone,\n",
    "                ' '.join(products),\n",
    "                row['file_name']\n",
    "            ))\n",
    "        name = None\n",
    "        address_lines = []\n",
    "        order_id = None\n",
    "        phone = None\n",
    "        products = []\n",
    "\n",
    "    result_df = pd.DataFrame(extracted_data, columns=[\n",
    "        'Name', 'Full Address', 'Address1', 'Address2', 'Address2 No space',\n",
    "        'Address3', 'Address3 No space', 'Address4', 'Address4 No space', \n",
    "        'Address5', 'Address6', 'Address7', 'Order ID', 'Phone', 'Product', 'Label File Name'\n",
    "    ])\n",
    "    return result_df\n",
    "\n",
    "# Function to process all PDF files from Google Drive\n",
    "def process_all_pdfs_from_drive(service, file_list):\n",
    "    combined_df = pd.DataFrame(columns=['pdf_extract', 'file_name'])\n",
    "    for file in file_list:\n",
    "        file_id = file['id']\n",
    "        file_name = file['name']\n",
    "        request = service.files().get_media(fileId=file_id)\n",
    "        pdf_stream = io.BytesIO()\n",
    "        downloader = MediaIoBaseDownload(pdf_stream, request)\n",
    "        done = False\n",
    "        while not done:\n",
    "            status, done = downloader.next_chunk()\n",
    "            print(f\"Reading {file_name} {int(status.progress() * 100)}% complete.\")\n",
    "        pdf_stream.seek(0)\n",
    "        full_text = extract_text_from_pdf_stream(pdf_stream)\n",
    "        print(f\"Full text for {file_name}:\\n{full_text[:500]}\")  # Print first 500 characters for inspection\n",
    "\n",
    "        # Improved splitting logic\n",
    "        segments = full_text.strip().split(\"จาก\")\n",
    "        segments = segments[1:]  # Skip the first segment if it doesn't contain useful data\n",
    "        df = pd.DataFrame(columns=['pdf_extract', 'file_name'])\n",
    "        for segment in segments:\n",
    "            segment = \"จาก\" + segment  # Add back the removed split part\n",
    "            data = segment.strip().split('\\n')\n",
    "            new_row = pd.DataFrame({'pdf_extract': ['\\n'.join(data)], 'file_name': [file_name]})\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    print(\"Combined DataFrame:\\n\", combined_df.head())  # Inspect combined DataFrame\n",
    "    result_df = extract_name_address_order_phone_and_products(combined_df)\n",
    "    print(\"Result DataFrame:\\n\", result_df.head())  # Inspect result DataFrame\n",
    "    return result_df\n",
    "\n",
    "# Authenticate and create the service\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    CLIENT_SECRET_FILE, scopes=SCOPES)\n",
    "service = build(API_NAME, API_VERSION, credentials=credentials)\n",
    "client = gspread.authorize(credentials)\n",
    "\n",
    "# Google Drive folder ID\n",
    "folder_id = '1c0vP6Y9IjhD9zjc0hy1FVXchqGVP1-AN'\n",
    "query = f\"'{folder_id}' in parents\"\n",
    "\n",
    "# Initialize the list of files\n",
    "files = []\n",
    "\n",
    "# Get the first page of files\n",
    "response = service.files().list(q=query).execute()\n",
    "files.extend(response.get('files', []))\n",
    "nextPageToken = response.get('nextPageToken')\n",
    "\n",
    "# Loop through subsequent pages of files\n",
    "while nextPageToken:\n",
    "    response = service.files().list(q=query, pageToken=nextPageToken).execute()\n",
    "    files.extend(response.get('files', []))\n",
    "    nextPageToken = response.get('nextPageToken')\n",
    "\n",
    "# Process all PDFs from Google Drive\n",
    "result_df = process_all_pdfs_from_drive(service, files)\n",
    "\n",
    "# Add date and timestamp columns\n",
    "current_date = datetime.now().date()\n",
    "current_timestamp = datetime.now().isoformat()\n",
    "\n",
    "result_df['Date'] = current_date.strftime('%Y-%m-%d')\n",
    "result_df['Timestamp'] = current_timestamp\n",
    "\n",
    "columns_order = [\n",
    "    'Order ID', 'Name', 'Full Address', 'Address1', 'Address2', 'Address2 No space',\n",
    "    'Address3', 'Address3 No space', 'Address4', 'Address4 No space', 'Address5',\n",
    "    'Address6', 'Address7', 'Phone', 'Product', 'Label File Name', 'Date', 'Timestamp'\n",
    "]\n",
    "result_df = result_df[columns_order]\n",
    "\n",
    "# Append result to Google Sheets\n",
    "SHEET_URL = os.getenv(\"SHEET_URL\")\n",
    "SHEET_NAME = 'Label Extract MasterSheet'\n",
    "\n",
    "def append_to_sheet(sheet_url, sheet_name, new_data):\n",
    "    \"\"\"Append data to a Google Sheet without overwriting existing data.\"\"\"\n",
    "    sheet = client.open_by_url(sheet_url).worksheet(sheet_name)\n",
    "    \n",
    "    # Read existing data\n",
    "    existing_data = sheet.get_all_values()\n",
    "    if existing_data:\n",
    "        existing_df = pd.DataFrame(existing_data[1:], columns=existing_data[0])\n",
    "    else:\n",
    "        existing_df = pd.DataFrame(columns=new_data.columns)\n",
    "    \n",
    "    # Concatenate the existing data with the new data\n",
    "    combined_df = pd.concat([existing_df, new_data], ignore_index=True)\n",
    "    \n",
    "    # Sort by 'Label File Name' instead of renaming\n",
    "    combined_df = combined_df.sort_values(by='Label File Name', ascending=False)\n",
    "    \n",
    "    # Clear the existing sheet and update with the combined data\n",
    "    sheet.clear()\n",
    "    sheet.update([combined_df.columns.values.tolist()] + combined_df.values.tolist())\n",
    "\n",
    "append_to_sheet(SHEET_URL, SHEET_NAME, result_df)\n",
    "\n",
    "print(f\"Data appended to {SHEET_NAME} in the Google Sheets document.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging and Updating Order Data from Google Sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script effectively extracts data from Google Sheets, transforms it by merging and cleaning, and loads the transformed data back into a Google Sheet, making it a complete ETL process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Phase:\n",
    "\n",
    "Data Retrieval: The script connects to Google Sheets using the gspread library and retrieves data from two sheets: \"Update Order For Check Blacklist\" (UpdateSheet) and \"Label Extract MasterSheet\" (LabelSheet). This is done by using the get_all_records() method, which extracts all records from the sheets.\n",
    "\n",
    "Transform Phase:\n",
    "\n",
    "Data Conversion: The retrieved data is converted into pandas DataFrames (df1 and df2).\n",
    "Data Cleaning and Transformation:\n",
    "Order ID columns are converted to strings to handle large numbers.\n",
    "Specific columns from df2 are selected and renamed to avoid conflicts, creating df2_selected.\n",
    "The two DataFrames are merged on the Order ID column.\n",
    "Problematic values such as np.inf and -np.inf are replaced with None.\n",
    "Any non-serializable float values are converted to strings.\n",
    "A validation check (is_valid_dataframe) ensures there are no None values in the DataFrame.\n",
    "\n",
    "Load Phase:\n",
    "\n",
    "Data Backup: The existing data in UpdateSheet is backed up in case of failure.\n",
    "Data Update: If the DataFrame passes validation, the script clears UpdateSheet and writes the transformed data back to the sheet, ensuring all values are converted to strings for compatibility. If validation fails, the backup data is restored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the scope and credentials\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(CLIENT_SECRET_FILE, SCOPES)\n",
    "client = gspread.authorize(credentials)\n",
    "\n",
    "# Access the sheets using the URL\n",
    "spreadsheet_url = os.getenv('SHEET_URL')\n",
    "UpdateSheet = client.open_by_url(spreadsheet_url).worksheet('Update Order For Check Blacklist')\n",
    "LabelSheet = client.open_by_url(spreadsheet_url).worksheet('Label Extract MasterSheet')\n",
    "\n",
    "# Get the data\n",
    "data1 = UpdateSheet.get_all_records()\n",
    "data2 = LabelSheet.get_all_records()\n",
    "\n",
    "# Convert to DataFrames\n",
    "df1 = pd.DataFrame(data1)\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Convert 'Order ID' to string to handle large numbers\n",
    "df1['Order ID'] = df1['Order ID'].astype(str)\n",
    "df2['Order ID'] = df2['Order ID'].astype(str)\n",
    "\n",
    "# Select specific columns from df2 and rename to avoid conflicts\n",
    "df2_selected = df2[['Order ID', 'Full Address', 'Name', 'Address1', 'Address2', 'Address2 No space', 'Address3', 'Address3 No space', 'Address4', 'Address4 No space', 'Address5', 'Address6', 'Address7', 'Phone', 'Product', 'Label File Name']].rename(columns={\n",
    "    'Name': 'Label Full Name',\n",
    "    'Full Address': 'Label Full Address'\n",
    "})\n",
    "\n",
    "# Merge DataFrames on 'Order ID' and handle suffixes\n",
    "merged_df = pd.merge(df1, df2_selected, on='Order ID', how='left', suffixes=('', '_y'))\n",
    "\n",
    "# Handle problematic values\n",
    "# Replace inf, -inf with None\n",
    "merged_df.replace([np.inf, -np.inf], None, inplace=True)\n",
    "# Convert any non-serializable float values to strings\n",
    "merged_df = merged_df.applymap(lambda x: x if isinstance(x, (int, str)) else str(x) if isinstance(x, float) else x)\n",
    "\n",
    "# Check if the DataFrame contains any problematic data\n",
    "def is_valid_dataframe(df):\n",
    "    # Check for any rows with None values (or any other criteria you want)\n",
    "    return not df.isnull().values.any()\n",
    "\n",
    "# Backup existing data in case of failure\n",
    "backup_data = UpdateSheet.get_all_records()\n",
    "\n",
    "# Only update the sheet if the DataFrame is valid\n",
    "if is_valid_dataframe(merged_df):\n",
    "    # Clear UpdateSheet before writing the merged DataFrame\n",
    "    UpdateSheet.clear()\n",
    "    \n",
    "    # Convert all values to strings to ensure compatibility with Google Sheets\n",
    "    formatted_data = merged_df.applymap(lambda x: str(x) if pd.notnull(x) else '')\n",
    "\n",
    "    # Write the merged DataFrame back to UpdateSheet\n",
    "    UpdateSheet.update([formatted_data.columns.values.tolist()] + formatted_data.values.tolist())\n",
    "    print(\"Sheet updated successfully.\")\n",
    "else:\n",
    "    print(\"Data validation failed. Sheet not updated.\")\n",
    "    # Restore backup data if needed\n",
    "    backup_df = pd.DataFrame(backup_data)\n",
    "    UpdateSheet.update([backup_df.columns.values.tolist()] + backup_df.values.tolist())\n",
    "    print(\"Restoring backup data...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Sheets Data Integration and Reporting Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Data Processing:\n",
    "\n",
    "Reads data from two Google Sheets: one for orders and one for blacklisted records.\n",
    "Converts relevant columns to strings and checks for required columns.\n",
    "Matches records from the order sheet against the blacklist based on specified conditions.\n",
    "Reporting:\n",
    "\n",
    "Filters matched records, adds date and timestamp, and cleans up any problematic values.\n",
    "Updates the report sheet in Google Sheets by clearing old data and writing the new report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from google.oauth2.service_account import Credentials\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the URLs and sheet names\n",
    "UPDATE_SHEET_URL = os.getenv('UPDATE_SHEET_URL')\n",
    "UPDATE_SHEET_NAME = 'Update Order For Check Blacklist'\n",
    "BLACKLIST_SHEET_URL = os.getenv('BLACKLIST_SHEET_URL')\n",
    "BLACKLIST_SHEET_NAME = 'Piwpiw Bakery_ตีกลับ+Blscklist'\n",
    "REPORT_SHEET_URL = os.getenv('REPORT_SHEET_URL')\n",
    "REPORT_SHEET_NAME = 'Report Blacklist Update'\n",
    "\n",
    "\n",
    "def read_from_sheet(sheet_url, sheet_name):\n",
    "    creds = Credentials.from_service_account_file(CLIENT_SECRET_FILE, scopes=SCOPES)\n",
    "    client = gspread.authorize(creds)\n",
    "    sheet = client.open_by_url(sheet_url).worksheet(sheet_name)\n",
    "    data = sheet.get_all_records()\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Function to write data to a Google Sheet\n",
    "def write_to_sheet(sheet_url, sheet_name, df):\n",
    "    creds = Credentials.from_service_account_file(CLIENT_SECRET_FILE, scopes=SCOPES)\n",
    "    client = gspread.authorize(creds)\n",
    "    sheet = client.open_by_url(sheet_url).worksheet(sheet_name)\n",
    "    sheet.clear()\n",
    "    sheet.update([df.columns.values.tolist()] + df.fillna('').values.tolist())\n",
    "\n",
    "# Read data from the master sheet\n",
    "update_df = read_from_sheet(UPDATE_SHEET_URL, UPDATE_SHEET_NAME)\n",
    "\n",
    "# Read data from the blacklist sheet\n",
    "blacklist_df = read_from_sheet(BLACKLIST_SHEET_URL, BLACKLIST_SHEET_NAME)\n",
    "\n",
    "# Print column names for debugging\n",
    "print(\"Update DataFrame Columns:\", update_df.columns)\n",
    "print(\"Blacklist DataFrame Columns:\", blacklist_df.columns)\n",
    "\n",
    "update_df['Order ID'] = update_df['Order ID'].astype(str)\n",
    "blacklist_df['Order ID'] = blacklist_df['Order ID'].astype(str)\n",
    "\n",
    "# Ensure the required columns are present\n",
    "required_columns = ['Province', 'District', 'Label Full Name', 'Buyer Username']\n",
    "\n",
    "missing_update_columns = [col for col in required_columns if col not in update_df.columns]\n",
    "missing_blacklist_columns = [col for col in required_columns if col not in blacklist_df.columns]\n",
    "\n",
    "if missing_update_columns:\n",
    "    raise KeyError(f\"The following columns are missing in the update DataFrame: {missing_update_columns}\")\n",
    "if missing_blacklist_columns:\n",
    "    raise KeyError(f\"The following columns are missing in the blacklist DataFrame: {missing_blacklist_columns}\")\n",
    "\n",
    "# Check for matches based on the conditions\n",
    "condition_1 = update_df['Province'].isin(blacklist_df['Province']) & update_df['District'].isin(blacklist_df['District'])\n",
    "condition_2 = update_df['Label Full Name'].isin(blacklist_df['Label Full Name'])\n",
    "condition_3 = update_df['Buyer Username'].isin(blacklist_df['Buyer Username'])\n",
    "\n",
    "# Combine the conditions\n",
    "combined_condition = condition_1 | condition_2 | condition_3\n",
    "\n",
    "# Filter the update_df based on the combined condition\n",
    "report_blacklist = update_df[combined_condition].copy()\n",
    "current_date = datetime.now().date()\n",
    "current_timestamp = datetime.now().isoformat()\n",
    "\n",
    "report_blacklist.loc[:, 'Date'] = current_date.strftime('%Y-%m-%d')\n",
    "report_blacklist.loc[:, 'Timestamp'] = current_timestamp\n",
    "\n",
    "# Check for NaN or infinite values and replace them with empty strings\n",
    "report_blacklist.replace([pd.NA, float('inf'), float('-inf')], '', inplace=True)\n",
    "\n",
    "# Write the report_blacklist DataFrame to a Google Sheet\n",
    "write_to_sheet(REPORT_SHEET_URL, REPORT_SHEET_NAME, report_blacklist)\n",
    "print(f\"Finish Check blacklist and write report to {REPORT_SHEET_NAME}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
